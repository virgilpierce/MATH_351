{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "The goal with hypothesis testing is to construct a logical sequence of steps that fits into the probability computations we have been computing around point estimators and follows a standard pattern.\n",
    "\n",
    "## Statistical Tests\n",
    "\n",
    "The common elements for a test will be:\n",
    "\n",
    "- A Null Hypothesis: $H_0$\n",
    "- An Alternative Hypothesis:  $H_a$\n",
    "- A Test Statistic\n",
    "- A Rejection Region\n",
    "\n",
    "The Alternative Hypothesis must be the logical complement of the Null Hypothesis:  $H_a = \\neg H_0$\n",
    "\n",
    "The test is looking for sufficient evidence to accept the *Alternative Hypothesis*, in the absence of such evidence the *Null Hypothesis* is considered true. This distinction affects how we should construct the test - we choose for the Alternative Hypothesis the statement that needs evidence (the thing we are hoping to show).\n",
    "\n",
    "### Example: Car Accidents\n",
    "\n",
    "A city has made a major change to the timing of lights in the city to improve safety. Prior to this accidents in lighted intersections in the city fit a Poisson Distribution with a mean of 25 accidents per months. In the first month after the changes there were 8 accidents. Do we have sufficient evidence to conclude that interesections are safer?\n",
    "\n",
    "Notice we are asking:  \n",
    "\n",
    "- Could the 8 accidents or fewer just be the result of the Poisson Distribution with mean 25 and randomness (This will be our Null Hypothesis)\n",
    "- The alternative hypothesis is thus that the mean of the distribution has changed. Ideally for hypothesis testing in general we are confident that not so much has changed that the entire distribution has change.\n",
    "\n",
    "Note the choice: The alternative hypothesis is the one for which we are searching for sufficient evidence. Or to rephrase, the one for which we are trying to determine if the evidence we do have is significant. While the null hypothesis boils down to the evidence we do appear to have is just the result of the randomness in the problem.\n",
    "\n",
    "- The Test Statistics is the number of accidents in the month since the changes $Y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 25\n",
    "y = 12\n",
    "\n",
    "# This is our first time using the scipy.stats for a discrete variable. For discrete variables the method .pmf \n",
    "# is the probability mass function and gives the probability at a value.\n",
    "\n",
    "# The .cdf method is the cummulative distribution function and gives the value we expect P(Y \\leq y)\n",
    "\n",
    "poisson.cdf(y, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that the cdf is the \\leq variety\n",
    "\n",
    "sum( [ poisson.pmf(k, mu) for k in range(y+1) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that there is less than a 5% chance that with the mean of 25 the Poisson random variable would return a value of $Y \\leq 12$. We can conclude with 95% confidence that the modifications to the light timings has reduced the mean number of accidents per month. \n",
    "\n",
    "#### Rejection Region\n",
    "\n",
    "For the moment forget that we know what $Y$ is:\n",
    "\n",
    "- The Rejection Region are the values of $Y$ for which we would conclude there is sufficient evidence to reject the null hypothesis in favor of the alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ppf we can inquire of our distribution where the cdf crosses 0.05. \n",
    "\n",
    "poisson.ppf(0.05, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that at 17 the probability is bigger than 0.05 so we use 16.\n",
    "\n",
    "poisson.cdf(16, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so any value of $Y\\leq 16$ would lead to use rejecting the Null Hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I Errors: Rejecting $H_0$ when it is TRUE\n",
    "\n",
    "There are two ways we could make the wrong conclusion:  The first, called a Type I error would be that we reject the null hypothesis when it was in fact true. We use $\\alpha$ to denote the probability of a Type I error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type II Errors:  Accepting $H_0$ when it is FALSE\n",
    "\n",
    "Type II error is when we accept the null hypothesis but it is in fact false. We use $\\beta$ to denote the probability of a Type II error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "Taking a step back and looking at our test, a question is given the rejection region $Y\\leq 16$ how likely are the two errors for all possible $Y$ values?\n",
    "\n",
    "Type I Error:  The test is built for a 95% confidence and so the probability we incorrectly reject the null hypothesis is at most 5%. Note this is the parameter we can tune for the test.\n",
    "\n",
    "Type II Error:  This one takes some work to compute. It is the likliehood that the $Y$ was not in the rejection region (i.e. $Y \\geq 17$) and yet $H_0$ is false.\n",
    "\n",
    "$$ \\beta = P(\\mbox{type II error}) = P(Y \\geq 17 \\quad \\mbox{when the alternative hypothesis is true}) $$\n",
    "\n",
    "It is a little bit open ended because if the mean is not 25, we do not know what it is. However generally we expect that the smaller the difference between the true mean and 25 the greater the likliehood of a type II error. So to get a bound on $\\beta$, we will compute the above probability when the mean is something less than 25; generally I approach it as if we are rejecting $H_0$ and accepting $H_a$ then our best estimate for the new mean is from the sample we have just collected and so $\\mu_a = Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice this is then something we tune. \n",
    "# We choose the cutoff that balances the chance of a Type I and Type II error \n",
    "# to what we can accept. \n",
    "\n",
    "cutoff = 17\n",
    "alpha = poisson.cdf(cutoff, 25)\n",
    "beta = 1 - poisson.cdf(cutoff, cutoff)\n",
    "\n",
    "alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 50, 1)\n",
    "plt.figure(figsize = (10, 6) )\n",
    "\n",
    "plt.plot(x, poisson.pmf(x, 25), 'b-' )\n",
    "plt.plot(x, poisson.pmf(x, 17), 'r-' )\n",
    "plt.vlines(17, ymin = 0, ymax = poisson.pmf(17, 17), color='r', linestyle='dashed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is unsatisfacotry\n",
    "\n",
    "Play around with it a bit and you should notice that it appears to be substantially difficult to minimize both $\\alpha$ and $\\beta$ (especially with a Poisson or other discrete variable).\n",
    "\n",
    "So what are we to do?  Clearly what needs to happen is that we should increase the sample size (in this case we went with data from just one month). \n",
    "\n",
    "Increasing the sample size has two effects:  The Central Limit Theorem will start to apply as we sample from a distribution and then take the mean of the samples (if the mean is what our hypothesis is about). \n",
    "\n",
    "Increasing the sample size decreases the variance. \n",
    "\n",
    "----\n",
    "\n",
    "For the Poisson random variables the other way to think about increasing the sample size is that we are running the time period out longer getting a Poisson random variable with a larger mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean number of car accidents per year in this city's intersections is\n",
    "\n",
    "mu = 25*12\n",
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case our rejection region would be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson.ppf(0.05, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson.cdf(271, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 271\n",
    "alpha = poisson.cdf(cutoff, 300)\n",
    "beta = 1 - poisson.cdf(cutoff, cutoff)\n",
    "alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Sample Hypothesis Test Example\n",
    "\n",
    "Of course the real power comes as we do examples from unknown distributions but with a large enough size we are confident that the Central Limit Theorem applies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The claim is that students at a university are working no more than 20 hours per week jobs. To test this claim 25 students are selected at random and asked how many hours per week they are working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0, 5, 10, 12, 12, 12, 19, 19, 19, 19, 20, 20, 25, 30, 30, 35, 35, 35, 39, 39, 40, 41, 42, 50, 51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some items we need for our computation\n",
    "\n",
    "Ybar = np.mean(data)\n",
    "S = np.std(data, ddof = 1)\n",
    "n = len(data)\n",
    "Ybar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $H_0: \\mu \\leq 20$ and the $H_a: \\mu > 20$\n",
    "\n",
    "Our test statistic is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zstar = (Ybar - 20) / S * np.sqrt(n)\n",
    "Zstar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the p-value is $P( Z \\geq Z_\\star)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue = 1 - norm.cdf(Zstar)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "pvalue_t = 1 - t.cdf(Zstar, n-1)\n",
    "pvalue_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have insufficient evidence to reject the null the hypothesis at 99% confidence, but sufficient to reject it at 95% confidence. Note there is a difference between the t and normal distributions but it is not significant enough to change either conclusion. \n",
    "\n",
    "### Rejection Region\n",
    "\n",
    "Treating the $S$ as fixed we can ask what our rejection region would look like. I.e. for what value of $Z_\\star$ would we condlude that we reject the null hypothesis. At 95% confidence ($\\alpha = 0.05$) that region would be $\\bar{Y}$ bigger than:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = norm.ppf(0.95)*S/np.sqrt(n) + 20\n",
    "cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that we can then ask what the probability of a Type II error with this test would be. Again to answer this we need to have an alternative mean in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_a = 25\n",
    "zbeta = (cutoff - mu_a) / S * np.sqrt(n) \n",
    "zbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that with $Y < Y_{\\mbox{rejection}} $ we would incorrectly keep the null hypothesis because the true mean was 25 is then: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.cdf(zbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(15, 30, 0.1)\n",
    "plt.figure(figsize = (10, 6) )\n",
    "\n",
    "plt.plot(x, norm.pdf(x-20)*S/np.sqrt(n), 'b-' )\n",
    "plt.plot(x, norm.pdf(x-25)*S/np.sqrt(n), 'r-' )\n",
    "plt.vlines(cutoff, ymin = 0, ymax = norm.pdf(cutoff-25)*S/np.sqrt(n), color='r', linestyle='dashed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the test\n",
    "\n",
    "Note that the likliehood of a Type I error is set by the test, and from that we produce the rejection region. \n",
    "\n",
    "The Type II error is determine by first making a choice of what to assume the alternative hypothesis actually gives for a mean and then reporting the test statistic. Note that the opposite tail of the distribution is used. This choice of the alternative test value represents the **sensitivity** of the test.\n",
    "\n",
    "However in order to control $\\beta$ we notice that we have one more free variable we can tune:  $n$ the number of samples to collect. \n",
    "\n",
    "Varrying $n$ will have the effect of changing the location of the rejection region, and it will also, for a fixed $\\mu_a$ affect the size $\\beta$. Note it gets complicated because the rejection region also changes as we change $n$.\n",
    "\n",
    "Let $ z_{\\alpha} $ be chosen such that $P ( Z > z_\\alpha) = 0.05$ which we computed above using the .ppf() method.\n",
    "\n",
    "Then our rejection region is given by:\n",
    "\n",
    "$$ \\bar{Y} > z_\\alpha S / \\sqrt{n} + 20 $$\n",
    "\n",
    "Normalizing this with our alternative mean $25$ for a test sensitivity of 5 hours per week we get:\n",
    "\n",
    "$$ z_\\beta = \\sqrt{n} \\frac{z_\\alpha S / \\sqrt{n} + 20 - 25}{S} = z_\\alpha - \\sqrt{n} \\frac{5}{S} $$\n",
    "\n",
    "and we then choose a $n$ large enough that \n",
    "\n",
    "$$\\beta = P( Z < z_\\beta ) = 0.05 $$ \n",
    "\n",
    "You could do some algebra and find it exactly or just plug and check values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "zbeta = norm.ppf(0.95) - np.sqrt(n) * 5 / S\n",
    "beta = norm.cdf(zbeta)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(15, 30, 0.1)\n",
    "plt.figure(figsize = (10, 6) )\n",
    "\n",
    "plt.plot(x, norm.pdf(x-20)*S/np.sqrt(n), 'b-' )\n",
    "plt.plot(x, norm.pdf(x-25)*S/np.sqrt(n), 'r-' )\n",
    "plt.vlines(cutoff, ymin = 0, ymax = norm.pdf(cutoff-25)*S/np.sqrt(n), color='r', linestyle='dashed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has always surprised me that you do not actually need to take $n$ to be that much larger in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only wrinkle in practice is that because you are going to redo the experiment the value of $S$ is going to change. The estimate here for $\\beta$ is what we would call an *a priori* estimate, and you would want to redo this computation with the new $S$ to get an *a posteriori* estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity\n",
    "\n",
    "Note why we say that the difference between $\\mu_0$ and $\\mu_a$ is the test sensitivity. If we move $\\mu_a$ closer to $\\mu_0$ we would expect to need to take $\\sqrt{n}$ to be larger in order to decrese $\\beta$ as small. Note that once $\\mu_a = \\mu_0$ we would then have $z_\\beta = z_\\alpha$ and no freedom available.\n",
    "\n",
    "Try it by computing the $n$ needed if we want the test to be senstive to within 1 hour per week and $\\alpha = \\beta = 0.05$ or less.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

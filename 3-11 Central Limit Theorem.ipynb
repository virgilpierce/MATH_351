{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Limit Theorem\n",
    "\n",
    "Okay this is all great. We now have a method of looking at samples from the normal distirbution with an unkown mean $\\mu$ and using the sample mean $\\bar{Y}$ to build confidence intervals in which we believe the ture value of the mean should be. We also have a method of adjusting our expectations to cases where we do not know the true value of $\\sigma$. \n",
    "\n",
    "So that is all really great. However one might be getting worried at this point:\n",
    "\n",
    "\"Dr. Pierce. Are we going to have to do these calculations again for the Gamma Distribution and the Beta Distribution, and what about the Binomial and Poisson Distributions?\"\n",
    "\n",
    "To which, I can now let you know with great joy, that \"No\" we do not need to do that. And in fact it turns out that in most cases we do not even need to know what distribution we are even sampling from, provided that we have some freedom to choose a large enough sample.\n",
    "\n",
    "## From the homework\n",
    "\n",
    "You might recall, a couple of times now I have asked you to look at what happens as we sample from the exponential distribution.  Consider just using the sampling procedure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's draw samples of size 5 (a relatively small sample), compute their mean, and then bin the results:\n",
    "# from the exponential distribution with mean 15.\n",
    "\n",
    "result = []\n",
    "\n",
    "for k in range(1000):\n",
    "    sample = expon.rvs(size = 3)*15\n",
    "    result += [ sample.mean() ]\n",
    "    \n",
    "plt.hist(result, bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we get is a *mound* shape that is slightly skewed.\n",
    "\n",
    "What happens as we increase the size of the sample in the experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Always Happens\n",
    "\n",
    "The **Central Limit Theorem** states that:\n",
    "\n",
    "If $Y_1, Y_2, \\dots, Y_n$ are independent identically distributed random variables with $E(Y_i) = \\mu$ and $V(Y_i) = \\sigma^2$ then let\n",
    "\n",
    "$$ U_n = \\sqrt{n} \\frac{\\bar{Y} - \\mu}{\\sigma} $$\n",
    "\n",
    "Then the distribution of the $U_n$ converges to the standard normal distribution function as $n\\to \\infty$. I.e. \n",
    "\n",
    "$$ \\lim_{n\\to \\infty} P(U_n < u) = \\int_{-\\infty}^u \\frac{1}{\\sqrt{2\\pi}} e^{-t^2/2} dt $$\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Note a few things:\n",
    "\n",
    "- We do not need to know anything about the distribution of $Y$ except that it has a well defined mean and variance: for example the theorem fails if we try to use the uniformly distributed *real numbers*. \n",
    "- While the limit of the distributions of $U_n$ is given, the rate at which these distributions converge is not: in fact this rate will depend on the distribution of the $Y_i$. \n",
    "- You might wondering why we phrased the last part in terms of the cummulative distribution functions:  Well because the theorem is true when the $Y$ are discrete, and thus the $U_n$ are discrete and do not have a density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "In the 2020 NHL Season, teams averaged 2.9 goals scored per game, with a standard deviation of 0.41 goals. The *Tampa Bay Lightning* played 21 games and averaged 3.5 goals scored per game. Can we conclude from the goals scored per game that they were a significantly better team the rest of the league?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.sqrt(21)* (3.5 - 2.9)/0.41\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probability that in a randomly selected set of 21 games the number of goals scored would average\n",
    "# to more than 3.5 goals scored per game:\n",
    "\n",
    "1 - norm.cdf(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have therefor very strong evidence that the Tampa Bay team was better than average in the league.\n",
    "\n",
    "Another example from the same season:  the Dallas Stars played 19 games with 2.7 goals per game. Can we conclude that they were significantly worse than the league as a whole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.sqrt(19)*( 2.7 - 2.9)/0.41\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probability that in a radnomly selected set of 19 games the number of goals scored would average\n",
    "# to less than 2.7 goals scored per game: \n",
    "\n",
    "norm.cdf(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a somewhat more mixed result, though the probably of such an outcome is only 1.6%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson Distribution\n",
    "\n",
    "An interesting question is what the distribition of the number of goals scored in a game is, and its not hard to convince yourself that Poisson is probably the place to start. However in the context of the average number of goals over a set of $n$ games, the question of whether the distribution is Poisson or something else becomes less and less important as $n$ increases because of the Central Limit Theorem.\n",
    "\n",
    "With the example above, with $n = 19$ or $21$ we are in the region where treating the statistics as normally distributed may or may not be valid. Ideally we would have some additional evidence, maybe looking over multiple years of performance indicating that this is or is not a valid assumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of the Central Limit Theorem\n",
    "\n",
    "The proof relies on the following theorem:\n",
    "\n",
    "**Convergence of Moment Generating Functions** If $U_1, U_2, \\dots $ is a sequence of random variables with moment generating functions $m_1(t), m_2(t), \\dots,$ respectively, $U$ is a random variable with moment generating function $m(t)$ then if \n",
    "\n",
    "$$\\lim_{n\\to \\infty} m_n(t) = m(t) $$ for all $t$, then the distribution of $U_n$ converges to the distribution of $U$.\n",
    "\n",
    "This is not a shocking result so we skip a proof of it.\n",
    "\n",
    "### Proof of the Central Limit Theorem\n",
    "\n",
    "Given the conditions of the Central Limit Theorem, we have $$ U_n = \\sqrt{n} \\frac{\\bar{Y} - \\mu}{\\sigma} = \\frac{1}{\\sqrt{n}} \\sum Z_i $$ where \n",
    "$$ Z_i = \\frac{Y_i - \\mu}{\\sigma} $$\n",
    "\n",
    "Because the $Y_i$ are idependent and identically distributed we have the following result for the moment generating function of $\\sum Z_i$:\n",
    "\n",
    "$$ m_{\\sum Z_i}(t) = ( m_{Z_1}(t) )^n $$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$ m_{U_n}(t) = \\left( m_{Z_1}\\left(\\frac{t}{\\sqrt{n}}\\right) \\right)^n $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. So now let's see what we can say about $m_{Z_1}(t)$. \n",
    "\n",
    "We can use Taylors Theorem to give:\n",
    "\n",
    "$$ m_{Z_1}(t) = m_{Z_1}(0) + m_{Z_1}'(0) t + m_{Z_1}''(\\xi) \\frac{t^2}{2} $$ for some $\\xi \\in (0, t) $\n",
    "\n",
    "The helpful things are that we know the first two of these:  $$m_{Z_1}(0) = E(1) = 1$$ and $$m_{Z_1}'(0) = E(Z_1) = 0 $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ m_{U_n}(t) = \\left[ 1 + \\frac{m_{Z_1}''(\\xi_n) t^2 / 2}{n} \\right]^n $$ \n",
    "\n",
    "Noting that the $0 < \\xi_n < \\frac{t}{\\sqrt{n}}$ might be different for each $n$ but that they are also getting pinched to 0.\n",
    "\n",
    "We then observe that $$\\lim_{x\\to 0} m_{Z_1}''(x) = E(Z_1^2) = 1 $$\n",
    "\n",
    "Finally we need the classic result about the exponential that $$ \\lim_{n\\to \\infty} \\left[ 1 + \\frac{t^2/2}{n} \\right]^n = e^{t^2/2} $$\n",
    "\n",
    "Putting it all together we have:\n",
    "\n",
    "$$\\lim_{n\\to \\infty} m_{U_n}(t) = e^{t^2/2} $$\n",
    "\n",
    "The moment generating function of the standard normal random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example\n",
    "\n",
    "American households spent in 2019 an average of 64.83 dollars on movie streaming services with a standard deviation of 35 dollars. Note that the distribution of $Y$ the amount an American Household spends on movie streaming is almost surely not normally distributed - there are large numbers of people spending nothing on streaming.\n",
    "\n",
    "- So an example of question we could not answer without doing more to find this distribution would be:  What is the probability tha the Pierce family spends less than 20 dollars per year on movie streaming.\n",
    "\n",
    "- However the Central Limit Theorem tells us that if we average over lots of households, we expect that statistics to be normal.\n",
    "\n",
    "For example: A small town of 100 households spends on average 60 dollars per year on movie streaming services. Do we have evidence that this town does not represent a random sample of 100 households for streaming services expenses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3799999999999994"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.sqrt(100)* (60 - 64.83)/35\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0837933224150143"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "norm.cdf(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Binomial Distribution Revisited\n",
    "\n",
    "Our first example was the binomial distribution:  X is a 1 with probability p and 0 with probability 1-p and then the binomial variable is\n",
    "\n",
    "$$ Y = \\sum X_i $$\n",
    "\n",
    "for $X_i$ a sample of size n from the trial. \n",
    "\n",
    "Consequently this is more less a result that fits out *Central Limit Theorem*:  \n",
    "\n",
    "$$ \\frac{1}{n} Y = \\frac{1}{n} \\sum X_i$$\n",
    "\n",
    "is the average value of the $X_i$, and therefore as $n\\to \\infty$ we expect $1/n Y$ to approach a normal distribution.\n",
    "\n",
    "Check that: $ E(X) = p $ and $ V(X) = p (1-p) $\n",
    "\n",
    "\n",
    "Then:\n",
    "\n",
    "$$ U = \\sqrt{n} \\frac{ \\frac{1}{n} Y - p }{ \\sqrt{p (1-p)} } = \\frac{ Y - np}{ \\sqrt{p (1-p)} } $$\n",
    "\n",
    "is converging to a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "A 2015 survey of US college students indicated that 11.2% of 4-year college students faced food insequrity (defined as being unsure they would have enough food for their household in the next month or that they did not know when their next meal would be).\n",
    "\n",
    "There are 125 math majors at UNC. Find an upper bound that we are 90% confident the number of math majors with food insecurity is less than.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3263478740408408"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the Zscore for our 90% confidence upper bound:\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "b = norm.ppf(0.99)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that our CDF returns 90% for P(Z < 1.28)\n",
    "\n",
    "norm.cdf(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our 90% confidence interval for $Y$ is built from:\n",
    "\n",
    "$$ \\frac{Y - n p}{\\sqrt{ p (1-p) } } < 1.28 $$\n",
    "\n",
    "And then solving for $Y$ we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.133652078668725"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.112\n",
    "n= 75\n",
    "\n",
    "upper_bound = b * np.sqrt(p*(1-p)) + n*p\n",
    "upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n*p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Note how much easier that would be than building the distribution for $P(Y < 14)$. Even using a computer, this is a much faster process by appealing to the normal distribution. \n",
    "\n",
    "How large $n$ needs to be so that this normal approximation is valid depends on the value of p. The general rule of thumb is that \n",
    "\n",
    "$$ p \\pm 3 \\sqrt{pq/n} $$ should be in the interval $(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002753855903286001, 0.221246144096714)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the example above\n",
    "\n",
    "p - 3*np.sqrt( p*(1-p) / n), p + 3 *np.sqrt( p*(1-p) / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scores\n",
    "\n",
    "Standardized tests like the SAT and ACT are normalized over multiple years to a normal distribution. The Central Limit Theorem explains why one would expect a test with a large number of correct / wrong questions to have total scores that follow a normal distribution. \n",
    "\n",
    "The individual questions are the binomial trials, and so one might be able to convince themselves that the average number of correct questions should fit a normal distribution provided that the number of questions is large enough. \n",
    "\n",
    "What is the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

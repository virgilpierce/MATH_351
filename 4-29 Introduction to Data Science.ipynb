{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the https one if you are accessing the csv online\n",
    "# train_url = 'https://drive.google.com/uc?export=download&id=1SYAbRXOY96UqmOVLVqzG6J1koozpDQ29'\n",
    "\n",
    "# use the csv local if you have downloaded a copy of the csv data file\n",
    "train_url = 'train.csv'\n",
    "\n",
    "train = pa.read_csv(train_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition\n",
    "\n",
    "This dataset is a collection of images of hand drawn digits. The task being set is to develop a method for a computer to read the image and identify what number it is. The way we will try to do this is that we have divided our data into a training set and a testing set. We will use the training set to develop the algorithm and then the testing set to test how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(train.iloc[1, 1:]).reshape(28, 28), cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "This dataset is a classic example and was one of the first image centered machine learning problems, it was posed initially by the US Postal Service when they were developing their high speed automated sorting systems - they wanted a method of reading handwritten zip codes.  Colleagues of mine at the University of British Columbia used this testing and training set to develop an algorithm for reading student id numbers on exams for an online grading system they built.\n",
    "\n",
    "Note that accuracy is very important in these applications. An error rate of 0.01 on reading digits means that if we scan the five digits of a zip code the error rate on the zip code (the probability that at least one digit is wrong) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - 0.99**5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you starting using it for student id numbers or longer numbers the situation is even worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images as Data in the Sense of Probability and Statistics\n",
    "\n",
    "An interesting question is how do we think of images as Data in the sens of our MATH 351 class. Each image is a collection of intensity values for each pixel in the rectangle identifying how bright or dark that pixel should be. These images are 30x30 and so each one has $30^2 = 900 $ values associated with it.  We think of each image as a member of our sample and then the values for its pixes as the associated random variables. \n",
    "\n",
    "Immediately you see the fundamental difference between this problem and the ones in our class we have been doing. There is a lot more data for each member of the sample than we typically considered.\n",
    "\n",
    "You also see though that for example, we expect a high degree of correlation:  A pixel that is dark black is very likely to have neighbors that are dark black or at least not white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 10, figsize = (18, 8))\n",
    "\n",
    "for i in range(50):\n",
    "    ax[i%5][i//5].imshow( np.array(train.iloc[i, 1:]).reshape(28, 28), cmap = 'binary' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Because the size of the problem is so big, one needs some techniques for reducing the problem to the most important parts. One tool for doing this is **Principal Component Analsysis** which is a variation (or at least similar) to the eigenvalue decomposition you learned about in linear algebra. It determines which linear combinations of the pixels do the most to explain the variations in the values for the pixels. I.e. it tries to weed through the correlations and leave us with just the most explanatory values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we separate out the pixel values from the known value of the image:  \n",
    "# we are thinking now of the image X as the input and y as the output.\n",
    "\n",
    "X = np.array(train.iloc[:, 1:])\n",
    "y = np.array(train.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpca = PCA().fit(X)\n",
    "XPCA = xpca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will plot the first two principal components with one digit colored in blue and the others in red, that is theses are the two most descriptive features identified by the linear algebra computation done by the PCA algorithm. We note that for some of the digits notably 0, 1, 2, 3, and 4 it definitely appears that the first two components do separate them from the other digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, sharex='col', sharey='row', figsize = (14, 8))\n",
    "\n",
    "for i in range(10):\n",
    "    ax[i%2, i//2].plot(XPCA[y==i,0],XPCA[y==i,1],'b.')   \n",
    "    ax[i%2, i//2].plot(XPCA[y!=i,0],XPCA[y!=i,1],'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another excersise that illustrates what we have accomplished is to actually render the principal components as images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xpca.components_\n",
    "v = []\n",
    "for i in range(10):\n",
    "    v = v + [x[i].reshape(28, 28)]\n",
    "    \n",
    "fig, ax = plt.subplots(2, 5, figsize = (18, 8))\n",
    "\n",
    "for i in range(10):\n",
    "    ax[i%2][i//2].imshow(v[i], cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the languages connecting our class to this work:  Here we show that as we add the principle components in order we explain more and more of the variance in the data. The principal components are ordered and the first few explain much more of the variance than the later ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = (10, 8))\n",
    "plt.plot(np.cumsum(xpca.explained_variance_ratio_) )\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "So now the game is to develop tests that look at the data for an image and make a prediction about what number that image represents. They will do this by first computing the Principal Components for that image and then using those to make a prediction. We can speed this up by only using some of the principle components knowing that the first few of them go a long way to accounting for the variances in the images.\n",
    "\n",
    "The model we will use is called a *Neural Network* it is built out of cells that take the values for each principle component from our image and then combine them in cells; these cells then pass information to the next layer of cells, and so on until the final layer of cells produces an output of 0, 1, 2, ..., 9 and that is our prediction. We train the network by deciding how strong each connection from cell to cell is so that the algorithm does as good as it can on the training data, and then the test is how well it does with the testing data.\n",
    "\n",
    "What we actually do is repeat this testing and training process over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def runTest(clf, X, y, training=0.66, plotTest=True, pad=0.1):\n",
    "    \n",
    "    # Make a training and testing set for the method\n",
    "    perm = np.random.permutation(len(y))\n",
    "    n = X.shape[0]\n",
    "    trainSize = int(training*n)\n",
    "    Xtrain = X[perm[:trainSize], :]   # Use all of the columns passed\n",
    "    Xtest = X[perm[trainSize:], :]\n",
    "    \n",
    "    ytrain = y[perm[:trainSize]]\n",
    "    ytest = y[perm[trainSize:]]\n",
    "    \n",
    "    # Run the calculation\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "      \n",
    "    # Print out some metrics\n",
    "    print('Training Score: {}'.format(clf.score(Xtrain, ytrain)))\n",
    "    print('Testing Score: {}'.format(clf.score(Xtest, ytest)))\n",
    "    \n",
    "    for i in range(10):\n",
    "        print('Missed {}:  {}'.format(i, sum(clf.predict(Xtest[ytest!=i])==i)/sum(ytest!=i)))\n",
    "    \n",
    "    \n",
    "    # Returning the training and testing scores\n",
    "       \n",
    "    return clf.score(Xtrain, ytrain), clf.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=[100])\n",
    "\n",
    "runTest(mlp, XPCA[:, 0:100], y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course to find the best configuration for a particular problem requires some experimentation, and this is where the work from our class is worthwhile. We repeat the train/testing procedure above five times and collecting the *Testing Scores* in each case. This is done such that each data point is used once as a test value and 4 times as a training value. We can view the resulting test scores as a sample from some population. We want to understand the population mean and population variance of the testing error, and then we should choose the configuration of neurons that gives us the best performance of:  speed, testing score mean, and minimum likely testing score.\n",
    "\n",
    "Of course it is difficult to balance all of these, and that is where expertise, experience and a bit of artistry is needed. **YOU ARE NEEDED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(mlp, XPCA[:, 0:100], y, cv=5)\n",
    "mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=[100])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use what is call a confusion matrix to ask where the errors occur on the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = mlp.predict(XPCA[:, :100])\n",
    "y_true = y\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# print(cm)\n",
    "\n",
    "# plt.imshow(cm, cmap = 'ocean')\n",
    "# plt.colorbar\n",
    "\n",
    "min_val, max_val = 0, 15\n",
    "\n",
    "# intersection_matrix = np.random.randint(0, 10, size=(max_val, max_val))\n",
    "plt.figure(11)\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "# ax.matshow(cm, cmap=plt.cm.magma_r)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        c = cm[j,i]\n",
    "        ax.text(i, j, str(c), va='center', ha='center')\n",
    "\n",
    "\n",
    "plt.xticks(range(10))\n",
    "plt.yticks(range(10))\n",
    "plt.title('Confusion matrix',size = 28)\n",
    "plt.xlabel('True labeling',size = 20)\n",
    "plt.ylabel('Predicted labeling',size = 20)\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bayes Methods I am following Allen Downey's book *Think Bayes* which is available from OReilly press. It includes some notes on how to do these computations in Python that are unfortunately beyond what we have been doing in class (it includes the uses of classes, which I just don't want to get into this semester).\n",
    "\n",
    "# Conditional Probablity\n",
    "\n",
    "Recall that conditional probability is defined by:\n",
    "\n",
    "$$ P(A|B) = \\mbox{The likliehood of outcome A given outcome B has happened} $$\n",
    "\n",
    "Consider a simple example:  We have two bowls full of cookies (YUM!); bowl 1 has 30 vanilla cookies and 10 chocolate cookies, while bowl 2 has 20 of each flavor.\n",
    "\n",
    "How likely is it that we have:\n",
    "\n",
    "1. Selected bowl 1 by choosing one bowl at random?\n",
    "\n",
    "2. Selected a vanilla cooking by choosing one bowl at random and one cookie at random?  Are there different ways to think about this?\n",
    "\n",
    "3. Selected a vanilla cookie given that we choose bowl 1?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Selected a vanilla cookie and selected bowl 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the question that seems a little bit hard:  How likely is it that we have selected bowl 1 given that we selected a vanilla cookie?\n",
    "\n",
    "## Formula for Conditional Probability\n",
    "\n",
    "Our solution in 4 generalizes to:  \n",
    "\n",
    "$$ P(A \\quad \\mbox{and} \\quad B) = P(A) P(A | B) $$\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "However note the brilliant thing:  $ P(A \\quad \\mbox{and} \\quad B)$ is symmetric in $A$ and $B$ and so we must have:\n",
    "\n",
    "$$ P(B) P(B| A) = P( A \\quad \\mbox{and} \\quad B) = P(A) P(A| B) $$ \n",
    "\n",
    "which means we can use this to reverse the order of our conditions provided the other probabilities are known:\n",
    "\n",
    "$$ P(B | A) = \\frac{P(A) P(A|B)}{P(B)} $$\n",
    "\n",
    "and so our cookie question becomes computable:  How likely is it that we have selected bowl 1 given that we have selected a vanilla cookie?\n",
    "\n",
    "## An Information Theoretic Point of View\n",
    "\n",
    "Another way to think about what Bayes Theorem gives us. Suppose we have some hypothesis H we are testing. We have developed an estimate for P(H) using all of the information we have. We will call this our **prior estimate**.\n",
    "\n",
    "However then we obtain some new information, let's call it D for data.\n",
    "\n",
    "Our question is then how should our **prior estimate** be updated by this D?  Bayes Theorem tells us:\n",
    "\n",
    "$$ P( H | D ) = \\frac{ P(H) P(D|H) }{P(D) } $$\n",
    "\n",
    "- $P(H|D)$ is called our **posterior estimate**\n",
    "- $P(H)$ is our **prior estimate**\n",
    "- $P(D|H)$ is our **likliehood** of D under the hypothesis\n",
    "- $P(D)$ is the **normalizing constant** that is ideally the probability of $D$ under all possible hypotheses.\n",
    "\n",
    "Humans do this instinctively if imprecisely already. Essentially we are asking how likely was it that would observe $D$ if our hypothesis was true?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - M&Ms\n",
    "\n",
    "In 1995 Mars introduced blue M&Ms to their classic candy. Prior to 1995 bags of M&M had 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, and 10% Tan.\n",
    "\n",
    "After 1995 they had 24% Blue, 20% Green, 16% Orange, 14% Yellow, 13% Red, and 13% Brown.\n",
    "\n",
    "You are visiting the Mars Company Museum where they have two bags of M&Ms on display, one from 1994 and one from 1996, but with no labels. Against all the rules, probably because you have been corrupted by Virgil, you reach out open the bags and grab one M&M from each:  a yellow one and a green one. What is the probability that the yellow one came from the 1994 bag?\n",
    "\n",
    "It looks complicated because I have two values, but consider that if Bag 1 is from 1994 then Bag 2 is from 1996; and vice versa.  Bag 1 being the one that gave you a yellow M&M, and bag 2 being the one that gave you a green one. \n",
    "\n",
    "Let: \n",
    "\n",
    "- A: Bag 1 is from 1994 and Bag 2 is from 1996.\n",
    "- B: Bag 1 is from 1996 and Bag 2 is from 1994.\n",
    "- D: Bag 1 gave you a Yellow and Bag 2 gave you a Green.\n",
    "\n",
    "Interestingly, it is often that the normalizing constant that is the hardest thing to compute, however in this case we have enumerated all possible hypothesis A and B, and so the sume of the numerators $P(H) P(D|H)$ for both \n",
    "hypothesis will give $P(D)$.\n",
    "\n",
    "Then complete the following table:\n",
    "\n",
    "| H | Prior P(H) | Likliehood P(D given H) | P(H) P(D given H) | Posterior P(H given D) |\n",
    "| -- | ---- | ---- | ---- | ---- | \n",
    "| A | | | | | \n",
    "| B | | | | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Monty Hall\n",
    "\n",
    "The classic example is the Monty Hall Problem.  On the game show *Let's Make a Deal*, one of the games was that there was a new car hidden behind one of 3 doors. The contestant choose one of the doors. Monty would then open one of the other two doors to show that there was no car behind it, and then ask the contestant:  *would you like to change your guess?* \n",
    "\n",
    "The question that appears in almost every Probability and Stats Class is:  What should the contestant do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Dice\n",
    "\n",
    "Suppose I have a box of dice that contains a 3 sided die, a 6 sided die, an 8 sided die, a 12 sided die, and a 20 sided die. I selected a die from the box and roll it and get a 6. Find the probability that the die I selected is each of the dice in the box.\n",
    "\n",
    "You need to choose a representation for the hypotheses, a representation for the data, and then find the **prior**, **likliehood** and **posterior**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - The Locomotive Problem\n",
    "\n",
    "A variation on a problem from Frederick Mosteller's *Fifty Challenging Problems in Probability with Solutions*. A Mathematics Department has N sections of College Algebra labeled 1 through N. You drop by their campus and are taken to a course with the section number 15. What do we expect N to be?\n",
    "\n",
    "This one is different from the others in that we do not really know what the **prior** should be. Choose something and then proceed. Then we will need to ask what the consequence of our choice was.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion - The German Tank Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

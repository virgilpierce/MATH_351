{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student's t Distribution\n",
    "\n",
    "We left off last class with the following result:  Suppose that the $Y_1, Y_2, \\dots, Y_n$ is some sample of size $n$ pulled from a population that is normally distributed with an unkown mean $\\mu$ and unkown variance $\\sigma^2$. We then had the following results:\n",
    "\n",
    "## $\\bar{Y}$ \n",
    "\n",
    "The sample mean:  $$ \\bar{Y} = \\frac{1}{n} \\sum Y_i $$ \n",
    "\n",
    "is a normally distributed random variable with mean $\\mu$ and variance $\\sigma^2 /n$.\n",
    "\n",
    "## $S^2$ \n",
    "\n",
    "The sample variance:  $$ S^2 = \\frac{1}{n-1} \\sum (Y_i - \\bar{Y})^2 $$\n",
    "\n",
    "satisfies:  $$ \\frac{(n-1) S^2}{\\sigma^2} $$ \n",
    "\n",
    "is given by a $\\chi^2$ distribution with $n-1$ degrees of freedom. $S^2$ and $\\bar{Y}$ are also independent random variables.\n",
    "\n",
    "## $T$\n",
    "\n",
    "Again we would like to use the $Z$ statistic:\n",
    "\n",
    "$$ Z = \\sqrt{n} \\frac{(\\bar{Y} - \\mu)}{\\sigma} $$ \n",
    "\n",
    "to build confidence intervals about $\\bar{Y}$ in which we predict $\\mu$ will be because this will be a standard normal random variable. However the problem is we do not know $\\sigma$. \n",
    "\n",
    "We can try to use $S$ to replace $\\sigma$ but the issue is that while $\\sigma$ is fixed, $S$ is itself a random variable. Which leads us to \n",
    "\n",
    "$$ T = \\sqrt{n} \\frac{ ( \\bar{Y} - \\mu)}{S} $$ \n",
    "\n",
    "which is a random variable given by the Student's t distribution with $n-1$ degrees of freedom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "from scipy.stats import norm\n",
    "\n",
    "df = 2\n",
    "x = np.linspace(t.ppf(0.01, df),\n",
    "                t.ppf(0.99, df), 100)\n",
    "plt.plot(x, t.pdf(x, df),\n",
    "       'r-', lw=5, alpha=0.6, label='t pdf')\n",
    "plt.plot(x, norm.pdf(x), \n",
    "         'b-', lw=5, alpha =0.6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Students t distribution\n",
    "\n",
    "Student's t distribution captures the idea that because $S$ is itself moving, we have a bit less certainity about the result. Hence the tails of the distribution are larger and the center of the distribution is smaller. As the size of the sample increases this effect becomes less noticeable and the t distribution approaches the standard normal distirbution. More or less two things are happening as the sample increases - the $S^2$ is a better approximation of the $\\sigma^2$ and the variance of $\\bar{Y}$ is getting small. \n",
    "\n",
    "### Example\n",
    "\n",
    "We have the weight of five high school wrestlers: 150, 152, 155, 155, and 160. Assuming that these are sampled from a normal distirbution with unkown mean ùúá and unkown variance $\\sigma^2$. Let's use use the students t distribution to find an interval about $\\bar{Y}$ where we are 90% certain $\\mu$ is located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Ybar\n",
    "\n",
    "data = [150, 152, 155, 155, 160]\n",
    "Ybar = sum(data) / 5\n",
    "Ybar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute S^2\n",
    "\n",
    "S2 = sum( [(data[i] - Ybar)**2 for i in range(5) ] )/4\n",
    "S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.sqrt(S2)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the interval of the t distribution centered on the mean in which 90% of the values are contained.\n",
    "\n",
    "# We use ppf to find the inverse of the cdf: Note that you need to pass it the number of degrees of freedom.\n",
    "\n",
    "t.ppf(0.95, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is worth using the cdf to check that we have this correct:\n",
    "\n",
    "t.cdf(2.13185, 4) - t.cdf(-2.13185, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is worth pausing here and comparing this value to the standard normal value we would use if we knew sigma:\n",
    "\n",
    "norm.ppf(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So putting everything together, we have 90% confidence that the value of mu will satisfy:\n",
    "\n",
    "$$ -2.13185 < \\sqrt{5} \\frac{\\bar{Y} - \\mu}{S} < 2.13185 $$\n",
    "\n",
    "We then solve this inequality for $\\mu$:\n",
    "\n",
    "$$ \\frac{-2.13185 S}{\\sqrt{5}} + \\bar{Y} < \\mu < \\frac{2.13185 S}{\\sqrt{5}} + \\bar{Y} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-2.13185*S/np.sqrt(5) + Ybar, 2.13185*S/np.sqrt(5) + Ybar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note if we knew that S = \\sigma we would have found the interval from the standard normal result\n",
    "\n",
    "-1.65*S/np.sqrt(5) + Ybar, 1.65*S/np.sqrt(5) + Ybar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the effect of not knowing $\\sigma$ is increasing the size of our confidence interval by a bit less than 1.0 to both the left and the right. This is very sensistive to the degrees of freedom, and in practice we can usually forget about the t-distribution and just use a normal distribution once the sample is 30 or higher. \n",
    "\n",
    "On the other hand, that last statement is a relic of using tables for these computations. With a computer like we are using, there is no downside to just always using the t-distribution. It is not computationally more complex for our computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we mean by 90% Confident\n",
    "\n",
    "It is worth pausing here and discussing what we mean. The interval we have found represents the region where, 90% of the values of T will be. We can think of that as meaning, if we repeat this experiment 100 times, 90 of those times the value of $T$ will be between (-2.132, 2.132). So we can build from that the interval where we expect $\\mu$ to be from one of those experiments and while we can't say this **IS** where $\\mu$ is; what we can say is that if we do this process 100 times (i.e. build our sample and compute the itnerval), 90 of those times we expect the interval we construct to contain $\\mu$.\n",
    "\n",
    "**However** Note that this means 10 of those times we would (expect to) have an interval that **DOES NOT** contain $\\mu$. \n",
    "\n",
    "So there is an ethical question hiding in here:  Is 90% confident, confident enough?\n",
    "\n",
    "And as often happens in ethics and in the statistics and data science classes, the answer is *It depends*.  \n",
    "\n",
    "We should ask about the consequences if we are wrong, we should ask about how often we expect this experiment to be repeated, and we should ask how expensive the experiment can be.\n",
    "\n",
    "## Effect of Increase the Confidence\n",
    "\n",
    "We could of course increase the confidence level, and in some cases we would want to. Say we wanted to be 99% confident?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.ppf(0.995, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cdf(4.6041, 4) - t.cdf(-4.6041, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 is more than twice 2.13!  In order to be 90% confident we have to more than double the size of the resulting interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-4.6041*S/np.sqrt(5) + Ybar, 4.6041*S/np.sqrt(5) + Ybar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is wider than the range of values in our sample!  \n",
    "\n",
    "- Confidence level affects the size of the confidence interval, larger confidence is attained by a larger interval\n",
    "- The Students t-distribution has large tails and so for large confidences we expect the size of the interval to increase rapidly.\n",
    "- Increase the size of the sample improves things by moving the t-distribution closer to the normal distribution and decreasing the variance in the sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two variances\n",
    "\n",
    "Suppose that $W_1$ and $W_2$ are two indepdent $\\chi^2$-distributed random variables with $\\nu_1$ and $\\nu_2$ degrees of freedom. Then \n",
    "\n",
    "$$ F = \\frac{ W_1/\\nu_1}{W_2 / \\nu_2} $$ is said to have an F distribution with $\\nu_1$ numerator degrees of freedom and $\\nu_2$ denomoninator degrees of freedom. \n",
    "\n",
    "In practice this is used by connecting it back to the sample and population variances, the ratio of which is the respective $W_i/\\nu_i$ in this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "Show that if we take independent samples of size $n_1 = 4$ and $n_2 = 8$ from two normal populations with the same variance, then find the upper bound $b$ such that \n",
    "\n",
    "$$ P( \\frac{S_1^2}{S_2^2} < b ) = 0.95 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f\n",
    "\n",
    "f.ppf(0.95, 3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.cdf(4.347, 3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This then gives us a test of whether two populations have the different variances or not. For example:\n",
    "\n",
    "draw two small samples from two different normal distributions but with the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = norm.rvs(size = 4)*4 + 10\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = norm.rvs(size = 8)*4 + 100\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1bar = sum(data1)/4\n",
    "Y2bar = sum(data2)/8\n",
    "\n",
    "Y1bar, Y2bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = np.sqrt( sum( [ (data1[i] - Y1bar)**2 for i in range(4)])/3 )\n",
    "S2 = np.sqrt( sum( [ (data2[i] - Y2bar)**2 for i in range(8)])/7 )\n",
    "\n",
    "S1, S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we set up our probability above to test whether the ratio was too large not too small, so \n",
    "# shoudl take the ration of the larger sample variance over the smaller sample variance.  \n",
    "\n",
    "# Also note that this is a random process and we are 95% certain we have the right interval; that\n",
    "# means, if we repeat this experiement 100 times we expect to reach the wrong conclusion 5 times.\n",
    "\n",
    "S2**2 / S1**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is less than 4.35 we have insufficient evidence to conclude that the first variance is bigger than the second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas consider a 4 values sampled from a distribution with a different standard deviation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = norm.rvs(size = 4)*8 + 10\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y3bar = sum(data3)/4\n",
    "Y3bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3 = np.sqrt( sum( [ (data3[i] - Y3bar)**2 for i in range(4)])/3 )\n",
    "S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3**2 / S2**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Limit Theorem\n",
    "\n",
    "Okay this is all great. We now have a method of looking at samples from the normal distirbution with an unkown mean $\\mu$ and using the sample mean $\\bar{Y}$ to build confidence intervals in which we believe the ture value of the mean should be. We also have a method of adjusting our expectations to cases where we do not know the true value of $\\sigma$. \n",
    "\n",
    "So that is all really great. However one might be getting worried at this point:\n",
    "\n",
    "\"Dr. Pierce. Are we going to have to do these calculations again for the Gamma Distribution and the Beta Distribution, and what about the Binomial and Poisson Distributions?\"\n",
    "\n",
    "To which, I can now let you know with great joy, that \"No\" we do not need to do that. And in fact it turns out that in most cases we do not even need to know what distribution we are even sampling from, provided that we have some freedom to choose a large enough sample.\n",
    "\n",
    "## From the homework\n",
    "\n",
    "You might recall, a couple of times now I have asked you to look at what happens as we sample from the exponential distribution.  Consider just using the sampling procedure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's draw samples of size 5 (a relatively small sample), compute their mean, and then bin the results:\n",
    "# from the exponential distribution with mean 15.\n",
    "\n",
    "result = []\n",
    "\n",
    "for k in range(1000):\n",
    "    sample = expon.rvs(size = 3)*15\n",
    "    result += [ sample.mean() ]\n",
    "    \n",
    "plt.hist(result, bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we get is a *mound* shape that is slightly skewed.\n",
    "\n",
    "What happens as we increase the size of the sample in the experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Always Happens\n",
    "\n",
    "The **Central Limit Theorem** states that:\n",
    "\n",
    "If $Y_1, Y_2, \\dots, Y_n$ are independent identically distributed random variables with $E(Y_i) = \\mu$ and $V(Y_i) = \\sigma^2$ then let\n",
    "\n",
    "$$ U_n = \\sqrt{n} \\frac{\\bar{Y} - \\mu}{\\sigma} $$\n",
    "\n",
    "Then the distribution of the $U_n$ converges to the standard normal distribution function as $n\\to \\infty$. I.e. \n",
    "\n",
    "$$ \\lim_{n\\to \\infty} P(U_n < u) = \\int_{-\\infty}^u \\frac{1}{\\sqrt{2\\pi}} e^{-t^2/2} dt $$\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Note a few things:\n",
    "\n",
    "- We do not need to know anything about the distribution of $Y$ except that it has a well defined mean and variance: for example the theorem fails if we try to use the uniformly distributed *real numbers*. \n",
    "- While the limit of the distributions of $U_n$ is given, the rate at which these distributions converge is not: in fact this rate will depend on the distribution of the $Y_i$. \n",
    "- You might wondering why we phrased the last part in terms of the cummulative distribution functions:  Well because the theorem is true when the $Y$ are discrete, and thus the $U_n$ are discrete and do not have a density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "In the 2020 NHL Season, teams averaged 2.9 goals scored per game, with a standard deviation of 0.41 goals. The *Tampa Bay Lightning* played 21 games and averaged 3.5 goals scored per game. Can we conclude from the goals scored per game that they were a significantly better team the rest of the league?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.sqrt(21)* (3.5 - 2.9)/0.41\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probability that in a randomly selected set of 21 games the number of goals scored would average\n",
    "# to more than 3.5 goals scored per game:\n",
    "\n",
    "1 - norm.cdf(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have therefor very strong evidence that the Tampa Bay team was better than average in the league.\n",
    "\n",
    "Another example from the same season:  the Dallas Stars played 19 games with 2.7 goals per game. Can we conclude that they were significantly worse than the league as a whole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.sqrt(19)*( 2.7 - 2.9)/0.41\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probability that in a radnomly selected set of 19 games the number of goals scored would average\n",
    "# to less than 2.7 goals scored per game: \n",
    "\n",
    "norm.cdf(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a somewhat more mixed result, though the probably of such an outcome is only 1.6%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of the Central Limit Theorem\n",
    "\n",
    "The proof relies on the following theorem:\n",
    "\n",
    "**Convergence of Moment Generating Functions** If $U_1, U_2, \\dots $ is a sequence of random variables with moment generating functions $m_1(t), m_2(t), \\dots,$ respectively, $U$ is a random variable with moment generating function $m(t)$ then if \n",
    "\n",
    "$$\\lim_{n\\to \\infty} m_n(t) = m(t) $$ for all $t$, then the distribution of $U_n$ converges to the distribution of $U$.\n",
    "\n",
    "This is not a shocking result so we skip a proof of it.\n",
    "\n",
    "### Proof of the Central Limit Theorem\n",
    "\n",
    "Given the conditions of the Central Limit Theorem, we have $$ U_n = \\sqrt{n} \\frac{\\bar{Y} - \\mu}{\\sigma} = \\frac{1}{\\sqrt{n}} \\sum Z_i $$ where \n",
    "$$ Z_i = \\frac{Y_i - \\mu}{\\sigma} $$\n",
    "\n",
    "Because the $Y_i$ are idependent and identically distributed we have the following result for the moment generating function of $\\sum Z_i$:\n",
    "\n",
    "$$ m_{\\sum Z_i}(t) = ( m_{Z_1}(t) )^n $$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$ m_{U_n}(t) = \\left( m_{Z_1}\\left(\\frac{t}{\\sqrt{n}}\\right) \\right)^n $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. So now let's see what we can say about $m_{Z_1}(t)$. \n",
    "\n",
    "We can use Taylors Theorem to give:\n",
    "\n",
    "$$ m_{Z_1}(t) = m_{Z_1}(0) + m_{Z_1}'(0) t + m_{Z_1}''(\\xi) \\frac{t^2}{2} $$ for some $\\xi \\in (0, t) $\n",
    "\n",
    "The helpful things are that we know the first two of these:  $$m_{Z_1}(0) = E(1) = 1$$ and $$m_{Z_1}'(0) = E(Z_1) = 0 $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ m_{U_n}(t) = \\left[ 1 + \\frac{m_{Z_1}''(\\xi_n) t^2 / 2}{n} \\right]^n $$ \n",
    "\n",
    "Noting that the $0 < \\xi_n < \\frac{t}{\\sqrt{n}}$ might be different for each $n$ but that they are also getting pinched to 0.\n",
    "\n",
    "We then observe that $$\\lim_{x\\to 0} m_{Z_1}''(x) = E(Z_1^2) = 1 $$\n",
    "\n",
    "Finally we need the classic result about the exponential that $$ \\lim_{n\\to \\infty} \\left[ 1 + \\frac{t^2/2}{n} \\right]^n = e^{t^2/2} $$\n",
    "\n",
    "Putting it all together we have:\n",
    "\n",
    "$$\\lim_{n\\to \\infty} m_{U_n}(t) = e^{t^2/2} $$\n",
    "\n",
    "The moment generating function of the standard normal random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

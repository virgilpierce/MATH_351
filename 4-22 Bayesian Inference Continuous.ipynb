{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Examples of Bayesian Methods\n",
    "\n",
    "\n",
    "## Example - Unfair Coins\n",
    "\n",
    "Suppose we have an unfair coin that returns heads with some probability p. (before we get into it, it is worth noting that you can buy such coins from a magic or toy shop). Before we gather any data, *Bayes Thinking* says that we should think of $p$ as being a random variable that can take values between 0 and 1. We could take a prior such as $f(p) = 2p$.  Note we might choose this prior if we suspect the coin is favored towards heads before we flip it.\n",
    "\n",
    "In this case we are going to flip the coin once and call that our observation. So this is a case where we have a continuous prior and a discrete likliehood. We compute the likliehoods of observing a head or a tails:\n",
    "\n",
    "$$ L( x=\\mbox{heads} | p) = p \\qquad L(x = \\mbox{tails} | p) = 1-p $$\n",
    "\n",
    "Then our total probability of observing x is given by:\n",
    "\n",
    "$$ P(x=\\mbox{heads}) = \\int_0^1 L( x=\\mbox{heads} | p) f(p) dp = \\int_0^1 2 p^2 dp = \\frac{2}{3} $$\n",
    "\n",
    "$$ P(x=\\mbox{tails}) = \\int_0^1 L( x=\\mbox{tails} | p) f(p) dp = \\int_0^1 2 (1-p) p dp = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sp.Symbol('p')\n",
    "\n",
    "sp.integrate( 2*(1-p)*p, (p, 0, 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Posterior Estimate* in both cases then becomes:\n",
    "\n",
    "$$ f(p | x=\\mbox{heads}) = \\frac{L( x=\\mbox{heads}|p) f(p) }{ P(x=\\mbox{heads}) } = 3 p^2 $$\n",
    "\n",
    "and in the other case\n",
    "\n",
    "$$ f(p | x=\\mbox{tails}) = 6 (1-p)p $$\n",
    "\n",
    "Depending on what we flip this then updates our result. You could imagine that what we then do is repeat this as we continue to flip the coin and record the results, and it continues to give variations on the beta distributions.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Note that our prior estimate was weighted towards heads, and the total probability we compute reflected that with 2/3 to 1/3. \n",
    "\n",
    "Note that the two results we conclude with are divergent. One is the result of the flip being heads and other the result of the flip being tails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Prior \n",
    "\n",
    "As we've learned an important step in Bayesian Analysis is understanding the role of the prior for our particular problem. In the ideal case we have enough data that our prior is submerged by the likliehoods of the data. However if the sample size is fixed, we may not be able to avoid effects of the prior. In the case of our coin for example, our initial prior was weighted towards heads.\n",
    "\n",
    "We could try a *flat prior* which is just a unifromly distributed prior where every possibility is equally likely - again in the prior.\n",
    "\n",
    "$$ f(p) = 1$$\n",
    "\n",
    "Note that our likliehoods do not changed, however the total probabilities will:\n",
    "\n",
    "$$ P( x = \\mbox{heads}) = \\int_0^1 p dp = \\frac{1}{2} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ P( x = \\mbox{tails}) = \\int_0^1 (1-p) dp = \\frac{1}{2} $$\n",
    "\n",
    "Then we compute the posterior estimates:\n",
    "\n",
    "$$ f(p | x= \\mbox{heads}) = 2 p $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ f(p | x=\\mbox{tails}) = 2 (1- p) $$\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Note the impressive symmetry. \n",
    "\n",
    "Note that our posterior in the event that the first flip is a head is precisely the prior we started with. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Our Posterior to Make a Prediction\n",
    "\n",
    "So let's start with our flat prior, we flip the coin once and obtain a heads and our posterior estimate for the likliehood of p is now:  $$ f(p| x_1 = \\mbox{heads} ) = 2p $$. \n",
    "\n",
    "We flip the coin again and get another heads so that our updated posterior is now:\n",
    "\n",
    "$$ f(p | x_1 = x_2 = \\mbox{heads}) = 3 p^2 $$\n",
    "\n",
    "What do we think is going to happen next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likliehood that a third flip is a heads given the value of p then looks like:\n",
    "\n",
    "$$ p(x_3 = \\mbox{heads} | x_1=x_2 =\\mbox{heads}) = \\int L(x=\\mbox{heads} | p) f(p | x_1 = x_2 = \\mbox{heads}) = \\int_0^1 3 p^3 dp = \\frac{3}{4} $$\n",
    "\n",
    "and therefore\n",
    "\n",
    "\n",
    "$$ p(x_3 = \\mbox{tails} | x_1=x_2 =\\mbox{heads}) = \\frac{1}{4} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credible Interval\n",
    "\n",
    "Consider our first two flips being a heads starting from a flat prior:\n",
    "\n",
    "$$ f(p| x_1 = x_2 = \\mbox{heads}) = 3 p^2 $$\n",
    "\n",
    "Let's construct a 90% creditble interval for $p$. To do this we want to find an interval of $p$ space (in this case [0, 1]), (a, b) such that\n",
    "\n",
    "$$ \\int_a^b f(p|x_1 = x_2 = \\mbox{heads}) dp = 0.90 $$\n",
    "\n",
    "Note we can choose any a and b! What makes sense is to start our interval at the most likely value for p and then increase it in both directions until its mass is 0.90. Of course for our posterior the most likely $p$ is 1. So that will be our b as p cannot be bigger. Thus we choose a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.integrate( 3*p**2, (p, 0.6, 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is our coin unfair?\n",
    "\n",
    "Do we have enough evidence to conclude the coin is not fair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Radioactive Decay\n",
    "\n",
    "An unknown radioactive isotope has a lifetime modeled by an exponential distribution:  $$L(x|\\lambda) = \\lambda \\exp(-\\lambda x) $$ \n",
    "\n",
    "With mean lifetime $1/\\lambda$.  Suppose we detect a decay after x seconds. What is our estimate of the mean liftime for this isotope.\n",
    "\n",
    "### Choosing a Prior\n",
    "\n",
    "Note what we saw above, the prior we choose ended up being from the same class as the posterior, and in fact there was a whole chain of related priors and posteriors (or a tree more properly because it depended on the results of the flips) all coming from the Beta distribution. So one thing we might be looking for here is a prior PDF that is related to the underlying distribution, anticipating that this relationship may continue into the posterior.\n",
    "\n",
    "Putting that observation aside. The other thing we want is a prior that makes sense and captures the range of possibilities for $\\lambda$. Before any observation, the possibilities for $\\lambda$ are that it could be a any positive number. \n",
    "\n",
    "We could take as a start that $\\lambda$ is a random variable fitting an exponential distribution with mean 1:  \n",
    "\n",
    "$ f(\\lambda) = \\exp( - \\lambda) $ \n",
    "\n",
    "(note a good exam question would be:  why won't a flat prior work with this example?\n",
    "\n",
    "### Total Probability of the observation\n",
    "\n",
    "The total probability density of observation $x$ is then:\n",
    "\n",
    "$$ g(x) = \\int L(x|\\lambda) f(\\lambda) d\\lambda = \\int_0^\\infty \\lambda \\exp(-\\lambda x - \\lambda) d\\lambda $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get\n",
    "\n",
    "$$  = \\int_0^\\infty \\lambda \\exp(- (x+1) \\lambda) d\\lambda  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llambda = sp.Symbol('llambda')\n",
    "b = sp.Symbol('b')\n",
    "sp.integrate( llambda* sp.exp(-5*llambda), (llambda, 0, b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\n",
    "$$ g(x) = \\frac{1}{(x+1)^2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this with the likliehood and prior we have the posterior estimate predicated on observation $x$, again expressed as a density:\n",
    "\n",
    "$$ f(\\lambda | x) =\\frac{ L(x | \\lambda) f(\\lambda) }{g(x)} = \\lambda (x+1)^2 \\exp(-(x+1) \\lambda) $$\n",
    "\n",
    "Which is now a higher order gamma distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot for an observation x\n",
    "\n",
    "def prior(llambda):\n",
    "    \n",
    "    return np.exp( -llambda) \n",
    "\n",
    "def posterior(llambda, x=5):\n",
    "    \n",
    "    return llambda*(x+1)**2 * np.exp( -(x+1)*llambda )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llambda = np.arange(0, 20, 0.1)\n",
    "f  = prior(llambda)\n",
    "ff = posterior(llambda, x=5)\n",
    "plt.plot(llambda, f, 'b-')\n",
    "plt.plot(llambda, ff, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these are not probabilities of x, they are probabilities of $\\lambda$ and that the expected value of $x$ is $1/\\lambda$.\n",
    "\n",
    "Now suppose we ask, after observing a decay at x=5 what we expect the next observation to be using our posterior.\n",
    "\n",
    "$$ E(x_2) = \\int_0^\\infty \\int_0^\\infty x_2 L(x_2 | \\lambda) f(\\lambda | x_1=5) d\\lambda dx_2 $$\n",
    "\n",
    "$$ = \\int_0^\\infty \\int_0^\\infty 36 x_2 \\lambda^2 e^{ - \\lambda x_2 - 6 \\lambda } d\\lambda dx_2 $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = sp.Symbol('x2')\n",
    "llambda = sp.Symbol('\\lambda')\n",
    "b = sp.Symbol('b')\n",
    "\n",
    "sp.integrate( 36*x2*llambda**2 *sp.exp( - llambda*x2 - 6*llambda), (x2, 0, b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.integrate( 36* sp.exp(-6*llambda), (llambda, 0, b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credible Interval for $\\lambda$\n",
    "\n",
    "Take our posterior for $x=5$:\n",
    "\n",
    "$$ f(\\lambda | x=5) = 36\\lambda \\exp( -6\\lambda)$$\n",
    "\n",
    "Use this to find a 99% credible interval for $\\lambda$. Again this should be an interval (a, b) such that the density over it given by our posterior is 0.99 or greater.  Again we have all sorts of choices, but the most obvious would be to start at the maximum value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Calculus to find the maximum value\n",
    "\n",
    "f = 36 * llambda * sp.exp(-6*llambda)\n",
    "f.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llambda_max = 36/216\n",
    "llambda_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we start with lambda_max and then move down and up from there. Note that we do not want to go smaller than 0.\n",
    "sp.integrate( f, (llambda, 1/6 - 0.1, 1/6 + 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "That's the game. The only barrier is whether the integrals being computed can be found exactly or not. \n",
    "\n",
    "Some general parting thoughts about Bayesian Methods:\n",
    "\n",
    "- There are two points of view. One is that Bayesian Methods are an alternative way of thinking about statistics and experiments that is more intuitive and closer to what the meaning that a non-expert thinks we are using in describing conclusions.\n",
    "- Another is that Bayesian Methods are one more tool. They are a tool that is useful if you are going to try and make a prediction following an experiment (what happens next). \n",
    "- In practice one has many observations not just one or two, and there are methods for doing Bayesian computations using lot's of observations in one go. It is beyond the scope of this class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
